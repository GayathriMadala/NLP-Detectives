{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **LSA WITH COSINE SIMILARITY**\n"
      ],
      "metadata": {
        "id": "Wvn90QDG--xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The below code is plagiarism detection implementation without explicit preprocessing function on custom dataset**"
      ],
      "metadata": {
        "id": "88KGiHB1_Kjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utM2N_HW_nP_",
        "outputId": "38132d33-074e-4bce-fb89-87980f0a8aad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "    svd = TruncatedSVD(n_components=2)\n",
        "    if tfidf_matrix.shape == (2, 1):\n",
        "      return None\n",
        "    lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
        "    similarity = cosine_similarity(lsa_matrix[0].reshape(1, -1), lsa_matrix[1].reshape(1, -1))[0][0]\n",
        "    return similarity\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/plagiarism_dataset.csv')\n",
        "\n",
        "# similarity threshold\n",
        "threshold = 0.5\n",
        "\n",
        "# Calculate similarity and predict plagiarism\n",
        "predictions = []\n",
        "\n",
        "#print(df.head(2))\n",
        "ignored_idx = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    similarity = calculate_similarity(row['text1'], row['text2'])\n",
        "    if not similarity:\n",
        "      df.at[index,'is_plagiarized'] = np.nan\n",
        "      continue\n",
        "    is_plagiarized = int(similarity > threshold)\n",
        "    predictions.append(is_plagiarized)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "predicted = np.array(predictions)\n",
        "actual = df['is_plagiarized'].dropna()\n",
        "actual=actual.values\n",
        "\n",
        "accuracy = accuracy_score(actual, predicted)\n",
        "f1 = f1_score(actual, predicted)\n",
        "\n",
        "Text1=\"The quick brown fox jumps over the lazy dog.\"\n",
        "Text2=\"A swift fox of brown color leaps above a dog that is not active.\"\n",
        "similarity = calculate_similarity(Text1, Text2)\n",
        "is_plagiarized = int(similarity > threshold)\n",
        "similarity_percentage = similarity * 100\n",
        "output = 'Plagiarized' if is_plagiarized == 1 else 'Not Plagiarized'\n",
        "print(f\"Similarity for the two input documents is {output} with a similarity score of: {similarity_percentage:.2f}%\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ree9X_QK_VFF",
        "outputId": "f955cea1-45bf-4ac2-f8ba-5b587c6deed0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity for the two input documents is Not Plagiarized with a similarity score of: 30.41%\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is plagiarism detection implementation with explicit preprocessing function on custom dataset along with additional lemmatization handled by TfidfVectorizer"
      ],
      "metadata": {
        "id": "37HU4zVnBXaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def preprocess(sentence):\n",
        "    \"\"\"Preprocess the sentence: lowercase, remove punctuation, tokenize.\"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "    return ''.join(sentence)\n",
        "\n",
        "def calculate_similarity_and_common_words(text1, text2):\n",
        "    \"\"\"Calculate cosine similarity between two texts and identify common high-scoring words.\"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "    # Find common words with the highest TF-IDF scores in both texts\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores_1 = tfidf_matrix[0].toarray().flatten()\n",
        "    tfidf_scores_2 = tfidf_matrix[1].toarray().flatten()\n",
        "\n",
        "    # Filter words by setting a threshold or consider all with non-zero in both\n",
        "    common_words_scores = {feature_names[i]: min(tfidf_scores_1[i], tfidf_scores_2[i])\n",
        "                           for i in range(len(feature_names))\n",
        "                           if tfidf_scores_1[i] > 0 and tfidf_scores_2[i] > 0}\n",
        "\n",
        "    # Sort by score\n",
        "    sorted_common_words = sorted(common_words_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return similarity, sorted_common_words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/plagiarism_dataset.csv')\n",
        "\n",
        "# Define similarity threshold\n",
        "threshold = 0.5\n",
        "\n",
        "# Calculate similarity and predict plagiarism\n",
        "predictions = []\n",
        "\n",
        "#print(df.head(2))\n",
        "ignored_idx = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    similarity,_ = calculate_similarity_and_common_words(row['text1'], row['text2'])\n",
        "    if not similarity:\n",
        "      df.at[index,'is_plagiarized'] = np.nan\n",
        "      continue\n",
        "    is_plagiarized = int(similarity > threshold)\n",
        "    predictions.append(is_plagiarized)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "predicted = np.array(predictions)\n",
        "actual = df['is_plagiarized'].dropna()\n",
        "actual=actual.values\n",
        "\n",
        "accuracy = accuracy_score(actual, predicted)\n",
        "f1 = f1_score(actual, predicted)\n",
        "\n",
        "Text1 = \"This is a simple test document.\"\n",
        "Text2 = \"This document is a test, not a real document.\"\n",
        "preprocessed_text1 = preprocess(Text1)\n",
        "preprocessed_text2 = preprocess(Text2)\n",
        "\n",
        "print(preprocessed_text1)\n",
        "print(preprocessed_text2)\n",
        "similarity, common_words = calculate_similarity_and_common_words(preprocessed_text1, preprocessed_text2)\n",
        "similarity_percentage = similarity * 100\n",
        "# Printing results\n",
        "print(f\"Similarity: {similarity_percentage:.2f}%\")\n",
        "\n",
        "if common_words:\n",
        "    print(\"Common words contributing to similarity:\")\n",
        "    for word, score in common_words:\n",
        "        print(f\"{word}\")\n",
        "else:\n",
        "    print(\"No common high-scoring words identified.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euZs_TG8BkIB",
        "outputId": "2180ad50-eabc-4558-9846-2c49befe95e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a simple test document\n",
            "this document is a test not a real document\n",
            "Similarity: 61.81%\n",
            "Common words contributing to similarity:\n",
            "document\n",
            "is\n",
            "test\n",
            "this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is plagiarism detection implementation with explicit preprocessing function on custom dataset along with additional lemmatization handled by TfidfVectorizer and POS Tagging"
      ],
      "metadata": {
        "id": "bq8BUAokB0LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import string\n",
        "import warnings\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "default_similarity_value = 1;\n",
        "\n",
        "# Download NLTK data (you may comment these out after the first run)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialize NLTK stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Helper function to map NLTK's POS tags to the format used by WordNetLemmatizer\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "# Function for basic text preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "\n",
        "    tagged_tokens = pos_tag(tokens)  # Get POS tags for tokens\n",
        "\n",
        "    # Remove stop words and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    \"\"\"Calculate cosine similarity between two texts.\"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "    svd = TruncatedSVD(n_components=2)\n",
        "    if tfidf_matrix.shape == (2, 1):\n",
        "      return None\n",
        "    lsa_matrix = svd.fit_transform(tfidf_matrix)\n",
        "    similarity = cosine_similarity(lsa_matrix[0].reshape(1, -1), lsa_matrix[1].reshape(1, -1))[0][0]\n",
        "    return similarity\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/plagiarism_dataset.csv')\n",
        "\n",
        "# Define similarity threshold\n",
        "threshold = 0.5\n",
        "\n",
        "# Calculate similarity and predict plagiarism\n",
        "predictions = []\n",
        "\n",
        "#print(df.head(2))\n",
        "ignored_idx = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    similarity = calculate_similarity(row['text1'], row['text2'])\n",
        "    if similarity is None:\n",
        "      df.at[index,'is_plagiarized'] = np.nan\n",
        "      continue\n",
        "    is_plagiarized = int(similarity > threshold)\n",
        "    predictions.append(is_plagiarized)\n",
        "\n",
        "# Evaluate model\n",
        "predicted = np.array(predictions)\n",
        "actual = df['is_plagiarized'].dropna()\n",
        "actual=actual.values\n",
        "\n",
        "accuracy = accuracy_score(actual, predicted)\n",
        "f1 = f1_score(actual, predicted)\n",
        "\n",
        "Text1=\"The paint on the canvas looks vibrant.\"\n",
        "Text2=\"They paint the wall with vibrant colors.\"\n",
        "preprocessed_text1 = preprocess_text(Text1)\n",
        "preprocessed_text2 = preprocess_text(Text2)\n",
        "print(preprocessed_text1)\n",
        "print(preprocessed_text2)\n",
        "similarity = calculate_similarity(preprocessed_text1, preprocessed_text2)\n",
        "if similarity is None:\n",
        "  similarity = default_similarity_value\n",
        "is_plagiarized = int(similarity > threshold)\n",
        "similarity_percentage = similarity * 100\n",
        "output = 'Plagiarized' if is_plagiarized == 1 else 'Not Plagiarized'\n",
        "print(f\"Similarity for the two input documents is {output} with a similarity score of: {similarity_percentage:.2f}%\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt1JfgKfB7Te",
        "outputId": "4056c260-c231-44ed-8a5e-891f7a989170"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paint canvas look vibrant\n",
            "paint wall vibrant color\n",
            "Similarity for the two input documents is Not Plagiarized with a similarity score of: 33.61%\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_uUSYR63ML9"
      },
      "source": [
        "## **MACHINE LEARNING MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SVM and Random Forest Classifier with custom dataset***"
      ],
      "metadata": {
        "id": "xAxXyH_3CMXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# Ensure you have the necessary packages\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/plagiarism_dataset.csv')  # Make sure to provide the correct path to your CSV file\n",
        "\n",
        "# Preprocessing: Tokenize the text\n",
        "df['text1_tokens'] = df['text1'].apply(lambda x: word_tokenize(x.lower()))\n",
        "df['text2_tokens'] = df['text2'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Combine all tokens for Word2Vec training\n",
        "all_tokens = pd.concat([df['text1_tokens'], df['text2_tokens']]).tolist()\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=all_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Define function to create a feature vector for each text by averaging all word vectors\n",
        "def document_vector(word_list, model):\n",
        "    # remove out-of-vocabulary words\n",
        "    word_list = [word for word in word_list if word in model.wv.index_to_key]\n",
        "    if len(word_list) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    else:\n",
        "        return np.mean(model.wv[word_list], axis=0)\n",
        "\n",
        "# Create vectors for each text\n",
        "df['text1_vector'] = df['text1_tokens'].apply(lambda x: document_vector(x, word2vec_model))\n",
        "df['text2_vector'] = df['text2_tokens'].apply(lambda x: document_vector(x, word2vec_model))\n",
        "\n",
        "# Prepare feature vectors for model training (subtracting vectors to get the difference)\n",
        "X = np.array(df.apply(lambda row: row['text1_vector'] - row['text2_vector'], axis=1).tolist())\n",
        "y = df['is_plagiarized'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "svm_predictions = svm_clf.predict(X_test)\n",
        "rf_predictions = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"SVM Classifier Report\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "\n",
        "print(\"Random Forest Classifier Report\")\n",
        "print(classification_report(y_test, rf_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HjpFzXgCVCx",
        "outputId": "fb595de4-ae31-44c6-d3ef-692378531be5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.31      0.48       191\n",
            "           1       0.70      1.00      0.83       309\n",
            "\n",
            "    accuracy                           0.74       500\n",
            "   macro avg       0.85      0.66      0.65       500\n",
            "weighted avg       0.82      0.74      0.69       500\n",
            "\n",
            "Random Forest Classifier Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       191\n",
            "           1       1.00      1.00      1.00       309\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       1.00      1.00      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess and tokenize new text\n",
        "def preprocess_and_tokenize(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "# Function to create a feature vector for prediction\n",
        "def create_feature_vector(text1, text2, model):\n",
        "    text1_vector = document_vector(preprocess_and_tokenize(text1), model)\n",
        "    text2_vector = document_vector(preprocess_and_tokenize(text2), model)\n",
        "    return text1_vector - text2_vector\n",
        "\n",
        "# Two new input texts\n",
        "new_text1 = \"An example of text to be checked for plagiarism.\"\n",
        "new_text2 = \"A text that is to be compared with the previous one for plagiarism.\"\n",
        "\n",
        "# Create feature vector for the new texts\n",
        "new_feature_vector = create_feature_vector(new_text1, new_text2, word2vec_model).reshape(1, -1)\n",
        "\n",
        "# Predict with SVM\n",
        "svm_prediction = svm_clf.predict(new_feature_vector)\n",
        "print(f\"SVM Prediction for plagiarism: {'Plagiarized' if svm_prediction[0] == 1 else 'Not Plagiarized'}\")\n",
        "\n",
        "# Predict with Random Forest\n",
        "rf_prediction = rf_clf.predict(new_feature_vector)\n",
        "print(f\"Random Forest Prediction for plagiarism: {'Plagiarized' if rf_prediction[0] == 1 else 'Not Plagiarized'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cJXm7AcDLIQ",
        "outputId": "49eaf9b2-eb25-4749-c8c9-819c6eb12ca3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Prediction for plagiarism: Plagiarized\n",
            "Random Forest Prediction for plagiarism: Not Plagiarized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM and Random Forest Classifier with custom dataset(still in progress)**"
      ],
      "metadata": {
        "id": "YveGv9wYCVfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oT0O_nbq3QEk",
        "outputId": "79317fae-6cb6-470b-da2f-9f140213c2c4"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-564067ddb5f9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nltk.download('punkt')  # Make sure this is uncommented if 'punkt' tokenizer is not already downloaded\n",
        "\n",
        "def read_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def parse_xml_for_plagiarism(xml_file_path):\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    plagiarism_cases = []\n",
        "    for feature in root.iter('feature'):\n",
        "        if feature.attrib.get('type') == 'plagiarism':\n",
        "            plagiarism_cases.append({\n",
        "                'source_offset': int(feature.attrib['source_offset']),\n",
        "                'source_length': int(feature.attrib['source_length']),\n",
        "                'suspicious_offset': int(feature.attrib['this_offset']),\n",
        "                'suspicious_length': int(feature.attrib['this_length']),\n",
        "                'source_reference': feature.attrib['source_reference']\n",
        "            })\n",
        "    return plagiarism_cases\n",
        "\n",
        "# Define the base path to the dataset\n",
        "base_path = 'https://drive.google.com/drive/folders/1gwU9kIeii-VLU7SO8pi-ZKG12kXZKhk2?usp=sharing'\n",
        "\n",
        "# Define paths for suspicious and source documents\n",
        "suspicious_path = os.path.join(base_path, 'suspicious-document')\n",
        "source_path = os.path.join(base_path, 'source-document')\n",
        "\n",
        "# Placeholder for data\n",
        "text_pairs = []\n",
        "\n",
        "# Iterate through the dataset directory\n",
        "for root, dirs, files in os.walk(suspicious_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.txt'):\n",
        "            suspicious_file_path = os.path.join(root, file)\n",
        "            suspicious_text = read_text(suspicious_file_path)\n",
        "\n",
        "            # Corresponding XML file\n",
        "            xml_file_path = suspicious_file_path.replace('.txt', '.xml')\n",
        "            plagiarism_cases = parse_xml_for_plagiarism(xml_file_path)\n",
        "\n",
        "            # For each case of plagiarism\n",
        "            for case in plagiarism_cases:\n",
        "                # Calculate the character positions for the plagiarized section\n",
        "                start_pos = case['suspicious_offset']\n",
        "                end_pos = start_pos + case['suspicious_length']\n",
        "                suspicious_excerpt = suspicious_text[start_pos:end_pos]\n",
        "\n",
        "                # Read the corresponding source text\n",
        "                source_file_path = os.path.join(source_path, case['source_reference'] + '.txt')\n",
        "                source_text = read_text(source_file_path)\n",
        "                source_start_pos = case['source_offset']\n",
        "                source_end_pos = source_start_pos + case['source_length']\n",
        "                source_excerpt = source_text[source_start_pos:source_end_pos]\n",
        "\n",
        "                # Tokenize and preprocess both excerpts\n",
        "                suspicious_tokens = word_tokenize(suspicious_excerpt)\n",
        "                source_tokens = word_tokenize(source_excerpt)\n",
        "\n",
        "                # Append to the dataset (token lists can be directly used for Word2Vec training)\n",
        "                text_pairs.append((source_tokens, suspicious_tokens))\n",
        "\n",
        "# Now text_pairs contains tokenized source and suspicious excerpts ready for vectorization\n",
        "\n",
        "# The next steps would involve vectorizing the text pairs and training your machine learning models,\n",
        "# similar to the previously provided examples.\n",
        "\n",
        "# Continue with Word2Vec training, vectorization, and machine learning model training...\n",
        "\n",
        "# Flatten the list of tokenized texts to train Word2Vec\n",
        "flat_tokenized_texts = [token for pair in text_pairs for text in pair for token in text]\n",
        "\n",
        "# Train a Word2Vec model on the corpus\n",
        "word2vec_model = Word2Vec(sentences=flat_tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Define a function to create a feature vector for a text pair by averaging all word vectors\n",
        "def feature_vector(text_pair, model):\n",
        "    source_vector = np.mean([model.wv[token] for token in text_pair[0] if token in model.wv], axis=0)\n",
        "    suspicious_vector = np.mean([model.wv[token] for token in text_pair[1] if token in model.wv], axis=0)\n",
        "    return np.abs(source_vector - suspicious_vector)\n",
        "\n",
        "# Create feature vectors for the dataset\n",
        "feature_vectors = np.array([feature_vector(pair, word2vec_model) for pair in text_pairs])\n",
        "\n",
        "# Define labels for the dataset: 1 for plagiarism, 0 for non-plagiarism\n",
        "labels = [1 if pair else 0 for pair in text_pairs]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Train a SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with both classifiers\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "svm_predictions = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifiers\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, svm_predictions))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mIhbNXJT_mC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}